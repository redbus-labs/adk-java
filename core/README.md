# Agent Development Kit (ADK) for Java
## Setting up PostgreSQL as a persistence store
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![r/agentdevelopmentkit](https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&logo=reddit&logoColor=white)](https://www.reddit.com/r/agentdevelopmentkit/)

--------------------------------------------------------------------------------

## âœ¨ Key Features

-   **External Configuration**: The database connection details are defined as constants, with a clear note that in a production environment, these should be managed through secure environment variables to protect sensitive information.

### 1. Install PostgreSQL

Refer to the [official documentation](https://www.postgresql.org/download/) for installation instructions for your platform.

### 2. Start PostgreSQL Service

```sh
# On macOS (Homebrew)
brew services start postgresql

# On Ubuntu/Debian
sudo service postgresql start
```

### 3. Create a Database

```sh
createdb adk_db
```

### 4. Connect to the Database

```sh
psql adk_db
```
## Table structure since in ADK will have Seesion,Events and event_content_parts

### 5. Create a session table for all the sessions.

```sql
CREATE TABLE sessions (
    id VARCHAR(255) PRIMARY KEY,       -- Unique identifier for the session, often a UUID or a unique string
    app_name VARCHAR(255) NOT NULL,    -- Name of the application associated with the session
    user_id VARCHAR(255) NOT NULL,     -- Identifier for the user who owns the session
    state JSONB DEFAULT '{}',
    event_data JSONB DEFAULT '[]',-- Current state of the session (e.g., JSON, text description)
    last_update_time TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP -- Timestamp of the last update, defaults to current time
);
```
--------------------------------------------------------------------------------
### 6. Create a events table for all the events 

```sql
CREATE TABLE IF NOT EXISTS events (
    id VARCHAR(255) PRIMARY KEY,-- This is also unique id generated by events
    session_id VARCHAR(255) NOT NULL, -- same as session id
    author VARCHAR(100),-- basically user or super agent or subagent
    actions_state_delta JSONB DEFAULT '{}',
    actions_artifact_delta JSONB DEFAULT '{}',
    actions_requested_auth_configs JSONB DEFAULT '{}',
    actions_transfer_to_agent VARCHAR(255),
    content_role VARCHAR(50),
    timestamp BIGINT,
    invocation_id VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT fk_events_session FOREIGN KEY (session_id)
        REFERENCES sessions(id) ON DELETE CASCADE
);
```
--------------------------------------------------------------------------------
### 7. Create a event_content_parts for all the sessions Table

```sql
CREATE TABLE IF NOT EXISTS event_content_parts (
    event_id VARCHAR(255) PRIMARY KEY,--Here to no bloating DB with duplicate insert using same event id as primary key
    session_id VARCHAR(255) NOT NULL,
    part_type VARCHAR(50),
    text_content TEXT,
    function_call_id VARCHAR(255),
    function_call_name VARCHAR(255),
    function_call_args JSONB,
    function_response_id VARCHAR(255),
    function_response_name VARCHAR(255),
    function_response_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT fk_parts_event FOREIGN KEY (event_id)
        REFERENCES events(id) ON DELETE CASCADE,
    CONSTRAINT fk_parts_session FOREIGN KEY (session_id)
        REFERENCES sessions(id) ON DELETE CASCADE
);
```
--------------------------------------------------------------------------------

### 8. Insert Data

```sql
ON CONFLICT (with column Name) we are using to avoid duplicates
```

### 9. Query Data

```sql
select * from sessions s 
select * from events
select * from event_content_parts ecp 
```

### *. Delete with cascade
delete from  sessions where id ='asasawe11'
This will take care of deleting from all other table in case needed.

--------------------------------------------------------------------------------

## ğŸš€ Kafka Consumer for Event Processing

The ADK includes a Kafka consumer service that reads adk events from Kafka and persists them to PostgreSQL database.

### Prerequisites for Kafka Consumer

1. **Environment Variables**: Set the following environment variables:
   ```bash
   export DBURL="your_database_url"
   export DBUSER="your_database_username" 
   export DBPASSWORD="your_database_password"
   ```

2. **Kafka Brokers**: Ensure Kafka brokers are running and accessible

3. **Database Tables**: The required tables (`sessions`, `events`, `event_content_parts`) should already exist from the setup above

### Configuration

The consumer reads configuration from `config.ini`:
```ini
[production]
kafkaBrokerAddress=localhost:9092,localhost:9093,localhost:9094
kafka_topic=adk-event
kafka_consumer_group=adk-event-consumer-group
```

### Running the Kafka Consumer

#### Method 1: Using Maven Exec Plugin (Recommended)
```bash
cd /path/to/adk-java
mvn exec:java -Dexec.mainClass="com.google.adk.kafka.consumer.KafkaConsumerRunner" -pl core
```

#### Method 2: With Custom Configuration
```bash
cd /path/to/adk-java
mvn exec:java -Dexec.mainClass="com.google.adk.kafka.consumer.KafkaConsumerRunner" -pl core -Dexec.args="/path/to/config.ini production"
```

#### Method 3: Compile and Run JAR
```bash
# Compile the project
cd /path/to/adk-java
mvn clean compile -pl core

# Run the compiled classes
java -cp "core/target/classes:core/target/dependency/*" com.google.adk.kafka.consumer.KafkaConsumerRunner
```

### What the Consumer Does

1. **Connects** to Kafka brokers specified in config
2. **Subscribes** to the `adk-event` topic
3. **Processes** incoming messages with `business_event = "adk-event"`
4. **Stores** data in PostgreSQL:
   - Session data in `sessions` table
   - Event data in `events` table  
   - Event content parts in `event_content_parts` table
5. **Handles** errors gracefully and continues processing
6. **Logs** all activities and errors

### Expected Output

When running successfully, you should see logs like:
```
[INFO] Starting Kafka Consumer Runner...
[INFO] Loading properties from: /path/to/adk-java/core/config.ini with environment: production
[INFO] Kafka consumer configuration loaded - Broker: localhost:9092,localhost:9093,localhost:9094, Topic: self_help_chat_events, Group: adk-event-consumer-group
[INFO] Consumer service initialized: KafkaConsumerService{broker='localhost:9092,localhost:9093,localhost:9094', topic='self_help_chat_events', group='adk-event-consumer-group', running=false}
[INFO] Kafka consumer service started successfully
[INFO] Kafka consumer is running. Press Ctrl+C to stop.
[INFO] Starting Kafka consumer loop
```

### Stopping the Consumer

- Press `Ctrl+C` to gracefully stop the consumer
- The consumer will finish processing current messages and then stop

### Troubleshooting

1. **Properties not loaded**: Ensure `config.ini` exists and is readable
2. **Database connection failed**: Check environment variables and database connectivity
3. **Kafka connection failed**: Verify Kafka brokers are running and accessible
4. **JSON parsing errors**: Check that incoming messages match expected format

### Monitoring

The consumer logs all activities including:
- Message processing success/failure
- Database operations
- Configuration loading
- Error conditions

Check the logs to monitor consumer health and performance.

### Kafka Consumer Architecture

```
Kafka Topic (adk_event)
    â†“
KafkaConsumerService
    â†“
KafkaEventRepository
    â†“
PostgreSQL Database
    â”œâ”€â”€ sessions table
    â”œâ”€â”€ events table
    â””â”€â”€ event_content_parts table
```

The consumer processes events in real-time and maintains data consistency across all three database tables.

--------------------------------------------------------------------------------

## ğŸ—ï¸ Multi-Tier Architecture Support

The ADK supports a multi-tier architecture with optional Redis caching and Kafka event streaming capabilities.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚    â”‚   PostgreSQL    â”‚    â”‚     Redis       â”‚
â”‚                 â”‚â”€â”€â”€â–¶â”‚   Database      â”‚    â”‚     Cache        â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka Event   â”‚    â”‚   Event Data    â”‚    â”‚   Fast Session  â”‚
â”‚   Streaming     â”‚    â”‚   Persistence   â”‚    â”‚   Retrieval     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Redis Caching Layer

Redis provides fast session caching for improved performance:

**Configuration:**
```ini
[production]
use_redis=true
redis_uri=redis://localhost:6379
```

**Benefits:**
- **Fast Session Retrieval**: Cached sessions reduce database load
- **Scalability**: Handle high-frequency session access
- **Performance**: Sub-millisecond response times for cached data

**How it works:**
1. Session data is written to both PostgreSQL and Redis
2. Frequent reads are served from Redis cache
3. Cache miss falls back to PostgreSQL database
4. Automatic cache invalidation on session updates

### Kafka Event Streaming

Kafka enables real-time event streaming for downstream consumers:

**Configuration:**
```ini
[production]
use_kafka=true
kafkaBrokerAddress=localhost:9092,localhost:9093,localhost:9094
kafkaTopic=adk-event
```

**Benefits:**
- **Real-time Processing**: Events are streamed immediately
- **Decoupling**: Downstream systems can process events independently
- **Scalability**: Handle high-volume event streams
- **Reliability**: Message persistence and delivery guarantees

**Event Flow:**
1. Session events are captured in real-time
2. Events are published to Kafka topics
3. Multiple consumers can process events independently
4. Event processing is asynchronous and non-blocking

### Configuration Options

**Full Multi-Tier Setup:**
```ini
[production]
# Database
db_url=jdbc:postgresql://localhost:5432/adk_db
db_user=postgres
db_password=postgres

# Redis Caching
use_redis=true
redis_uri=redis://localhost:6379

# Kafka Streaming
use_kafka=true
kafkaBrokerAddress=localhost:9092,localhost:9093,localhost:9094
kafkaTopic=adk-event

# Consumer Configuration
kafka_topic=adk-event
kafka_consumer_group=adk-event-consumer-group
```

**PostgreSQL Only (Minimal Setup):**
```ini
[production]
# Database only
db_url=jdbc:postgresql://localhost:5432/adk_db
db_user=postgres
db_password=postgres

# Disable optional features
use_redis=false
use_kafka=false
```

### Performance Benefits

| Feature | PostgreSQL Only | With Redis | With Kafka | Full Stack |
|---------|------------------|------------|------------|------------|
| Session Read | ~10ms | ~1ms | ~10ms | ~1ms |
| Session Write | ~5ms | ~5ms | ~5ms | ~5ms |
| Event Processing | Synchronous | Synchronous | Asynchronous | Asynchronous |
| Scalability | Limited | High | High | Very High |
| Reliability | High | High | High | Very High |

### Use Cases

**Redis Caching:**
- High-frequency session access
- Real-time applications
- Performance-critical systems
- Reduced database load

**Kafka Streaming:**
- Event-driven architectures
- Microservices communication
- Real-time analytics
- Audit logging
- Integration with external systems

**Combined Benefits:**
- **High Performance**: Fast reads from Redis
- **Real-time Processing**: Event streaming via Kafka
- **Data Persistence**: Reliable storage in PostgreSQL
- **Scalability**: Handle growing workloads
- **Flexibility**: Enable/disable features as needed
