/*
 * Copyright 2025 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.adk.flows.llmflows;

import com.google.adk.agents.ActiveStreamingTool;
import com.google.adk.agents.BaseAgent;
import com.google.adk.agents.CallbackContext;
import com.google.adk.agents.Callbacks.AfterModelCallback;
import com.google.adk.agents.Callbacks.BeforeModelCallback;
import com.google.adk.agents.Callbacks.OnModelErrorCallback;
import com.google.adk.agents.InvocationContext;
import com.google.adk.agents.LiveRequest;
import com.google.adk.agents.LlmAgent;
import com.google.adk.agents.ReadonlyContext;
import com.google.adk.agents.RunConfig.StreamingMode;
import com.google.adk.events.Event;
import com.google.adk.flows.BaseFlow;
import com.google.adk.flows.llmflows.RequestProcessor.RequestProcessingResult;
import com.google.adk.flows.llmflows.ResponseProcessor.ResponseProcessingResult;
import com.google.adk.models.BaseLlm;
import com.google.adk.models.BaseLlmConnection;
import com.google.adk.models.LlmCallsLimitExceededException;
import com.google.adk.models.LlmRegistry;
import com.google.adk.models.LlmRequest;
import com.google.adk.models.LlmResponse;
import com.google.adk.telemetry.Tracing;
import com.google.adk.tools.ToolContext;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.genai.types.FunctionResponse;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.StatusCode;
import io.opentelemetry.context.Context;
import io.opentelemetry.context.Scope;
import io.reactivex.rxjava3.core.Completable;
import io.reactivex.rxjava3.core.Flowable;
import io.reactivex.rxjava3.core.Maybe;
import io.reactivex.rxjava3.core.Single;
import io.reactivex.rxjava3.disposables.Disposable;
import io.reactivex.rxjava3.observers.DisposableCompletableObserver;
import io.reactivex.rxjava3.schedulers.Schedulers;
import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.atomic.AtomicReference;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** A basic flow that calls the LLM in a loop until a final response is generated. */
public abstract class BaseLlmFlow implements BaseFlow {
  private static final Logger logger = LoggerFactory.getLogger(BaseLlmFlow.class);

  protected final List<RequestProcessor> requestProcessors;
  protected final List<ResponseProcessor> responseProcessors;

  // Warning: This is local, in-process state that won't be preserved if the runtime is restarted.
  // "Max steps" is experimental and may evolve in the future (e.g., to support persistence).
  protected final int maxSteps;

  public BaseLlmFlow(
      List<RequestProcessor> requestProcessors, List<ResponseProcessor> responseProcessors) {
    this(requestProcessors, responseProcessors, /* maxSteps= */ Optional.empty());
  }

  public BaseLlmFlow(
      List<RequestProcessor> requestProcessors,
      List<ResponseProcessor> responseProcessors,
      Optional<Integer> maxSteps) {
    this.requestProcessors = requestProcessors;
    this.responseProcessors = responseProcessors;
    this.maxSteps = maxSteps.orElse(Integer.MAX_VALUE);
  }

  /**
   * Pre-processes the LLM request before sending it to the LLM. Executes all registered {@link
   * RequestProcessor} transforming the provided {@code llmRequestRef} in-place, and emits the
   * events generated by them.
   */
  protected Flowable<Event> preprocess(
      InvocationContext context, AtomicReference<LlmRequest> llmRequestRef) {
    LlmAgent agent = (LlmAgent) context.agent();

    RequestProcessor toolsProcessor =
        (ctx, req) -> {
          LlmRequest.Builder builder = req.toBuilder();
          return agent
              .canonicalTools(new ReadonlyContext(ctx))
              .concatMapCompletable(
                  tool -> tool.processLlmRequest(builder, ToolContext.builder(ctx).build()))
              .andThen(
                  Single.fromCallable(
                      () -> RequestProcessingResult.create(builder.build(), ImmutableList.of())));
        };

    Iterable<RequestProcessor> allProcessors =
        Iterables.concat(requestProcessors, ImmutableList.of(toolsProcessor));

    return Flowable.fromIterable(allProcessors)
        .concatMap(
            processor ->
                Single.defer(() -> processor.processRequest(context, llmRequestRef.get()))
                    .doOnSuccess(result -> llmRequestRef.set(result.updatedRequest()))
                    .flattenAsFlowable(
                        result -> result.events() != null ? result.events() : ImmutableList.of()));
  }

  /**
   * Post-processes the LLM response after receiving it from the LLM. Executes all registered {@link
   * ResponseProcessor} instances. Emits events for the model response and any subsequent function
   * calls.
   */
  protected Flowable<Event> postprocess(
      InvocationContext context,
      Event baseEventForLlmResponse,
      LlmRequest llmRequest,
      LlmResponse llmResponse) {

    List<Iterable<Event>> eventIterables = new ArrayList<>();
    Single<LlmResponse> currentLlmResponse = Single.just(llmResponse);
    for (ResponseProcessor processor : responseProcessors) {
      currentLlmResponse =
          currentLlmResponse
              .flatMap(response -> processor.processResponse(context, response))
              .doOnSuccess(
                  result -> {
                    if (result.events() != null) {
                      eventIterables.add(result.events());
                    }
                  })
              .map(ResponseProcessingResult::updatedResponse);
    }
    Context parentContext = Context.current();

    return currentLlmResponse.flatMapPublisher(
        updatedResponse -> {
          try (Scope scope = parentContext.makeCurrent()) {
            return buildPostprocessingEvents(
                updatedResponse, eventIterables, context, baseEventForLlmResponse, llmRequest);
          }
        });
  }

  /**
   * Sends a request to the LLM and returns its response.
   *
   * @param context The invocation context.
   * @param llmRequest The LLM request.
   * @param eventForCallbackUsage An Event object primarily for providing context (like actions) to
   *     callbacks. Callbacks should not rely on its ID if they create their own separate events.
   */
  private Flowable<LlmResponse> callLlm(
      InvocationContext context, LlmRequest llmRequest, Event eventForCallbackUsage) {
    LlmAgent agent = (LlmAgent) context.agent();

    LlmRequest.Builder llmRequestBuilder = llmRequest.toBuilder();

    return handleBeforeModelCallback(context, llmRequestBuilder, eventForCallbackUsage)
        .flatMapPublisher(
            beforeResponse -> {
              if (beforeResponse.isPresent()) {
                return Flowable.just(beforeResponse.get());
              }
              BaseLlm llm =
                  agent.resolvedModel().model().isPresent()
                      ? agent.resolvedModel().model().get()
                      : LlmRegistry.getLlm(agent.resolvedModel().modelName().get());
              return llm.generateContent(
                      llmRequestBuilder.build(),
                      context.runConfig().streamingMode() == StreamingMode.SSE)
                  .onErrorResumeNext(
                      exception ->
                          handleOnModelErrorCallback(
                                  context, llmRequestBuilder, eventForCallbackUsage, exception)
                              .switchIfEmpty(Single.error(exception))
                              .toFlowable())
                  .doOnNext(
                      llmResp ->
                          Tracing.traceCallLlm(
                              context,
                              eventForCallbackUsage.id(),
                              llmRequestBuilder.build(),
                              llmResp))
                  .doOnError(
                      error -> {
                        Span span = Span.current();
                        span.setStatus(StatusCode.ERROR, error.getMessage());
                        span.recordException(error);
                      })
                  .compose(Tracing.trace("call_llm"))
                  .concatMap(
                      llmResp ->
                          handleAfterModelCallback(context, llmResp, eventForCallbackUsage)
                              .toFlowable());
            });
  }

  /**
   * Invokes {@link BeforeModelCallback}s. If any returns a response, it's used instead of calling
   * the LLM.
   *
   * @return A {@link Single} with the callback result or {@link Optional#empty()}.
   */
  private Single<Optional<LlmResponse>> handleBeforeModelCallback(
      InvocationContext context, LlmRequest.Builder llmRequestBuilder, Event modelResponseEvent) {
    Event callbackEvent = modelResponseEvent.toBuilder().build();
    CallbackContext callbackContext =
        new CallbackContext(context, callbackEvent.actions(), callbackEvent.id());

    Maybe<LlmResponse> pluginResult =
        context.pluginManager().beforeModelCallback(callbackContext, llmRequestBuilder);

    LlmAgent agent = (LlmAgent) context.agent();

    List<? extends BeforeModelCallback> callbacks = agent.canonicalBeforeModelCallbacks();
    if (callbacks.isEmpty()) {
      return pluginResult.map(Optional::of).defaultIfEmpty(Optional.empty());
    }

    Maybe<LlmResponse> callbackResult =
        Maybe.defer(
            () ->
                Flowable.fromIterable(callbacks)
                    .concatMapMaybe(callback -> callback.call(callbackContext, llmRequestBuilder))
                    .firstElement());

    return pluginResult
        .switchIfEmpty(callbackResult)
        .map(Optional::of)
        .defaultIfEmpty(Optional.empty());
  }

  /**
   * Invokes {@link OnModelErrorCallback}s when an LLM call fails. If any returns a response, it's
   * used instead of the error.
   *
   * @return A {@link Maybe} with the override {@link LlmResponse}.
   */
  private Maybe<LlmResponse> handleOnModelErrorCallback(
      InvocationContext context,
      LlmRequest.Builder llmRequestBuilder,
      Event modelResponseEvent,
      Throwable throwable) {
    Event callbackEvent = modelResponseEvent.toBuilder().build();
    CallbackContext callbackContext =
        new CallbackContext(context, callbackEvent.actions(), callbackEvent.id());
    Exception ex = throwable instanceof Exception e ? e : new Exception(throwable);

    Maybe<LlmResponse> pluginResult =
        context.pluginManager().onModelErrorCallback(callbackContext, llmRequestBuilder, throwable);

    LlmAgent agent = (LlmAgent) context.agent();
    List<? extends OnModelErrorCallback> callbacks = agent.canonicalOnModelErrorCallbacks();

    if (callbacks.isEmpty()) {
      return pluginResult;
    }

    Maybe<LlmResponse> callbackResult =
        Maybe.defer(
            () -> {
              LlmRequest llmRequest = llmRequestBuilder.build();
              return Flowable.fromIterable(callbacks)
                  .concatMapMaybe(callback -> callback.call(callbackContext, llmRequest, ex))
                  .firstElement();
            });

    return pluginResult.switchIfEmpty(callbackResult);
  }

  /**
   * Invokes {@link AfterModelCallback}s after an LLM response. If any returns a response, it
   * replaces the original.
   *
   * @return A {@link Single} with the final {@link LlmResponse}.
   */
  private Single<LlmResponse> handleAfterModelCallback(
      InvocationContext context, LlmResponse llmResponse, Event modelResponseEvent) {
    Event callbackEvent = modelResponseEvent.toBuilder().build();
    CallbackContext callbackContext =
        new CallbackContext(context, callbackEvent.actions(), callbackEvent.id());

    Maybe<LlmResponse> pluginResult =
        context.pluginManager().afterModelCallback(callbackContext, llmResponse);

    LlmAgent agent = (LlmAgent) context.agent();
    List<? extends AfterModelCallback> callbacks = agent.canonicalAfterModelCallbacks();

    if (callbacks.isEmpty()) {
      return pluginResult.defaultIfEmpty(llmResponse);
    }

    Maybe<LlmResponse> callbackResult =
        Maybe.defer(
            () ->
                Flowable.fromIterable(callbacks)
                    .concatMapMaybe(callback -> callback.call(callbackContext, llmResponse))
                    .firstElement());

    return pluginResult.switchIfEmpty(callbackResult).defaultIfEmpty(llmResponse);
  }

  /**
   * Executes a single iteration of the LLM flow: preprocessing → LLM call → postprocessing.
   *
   * <p>Handles early termination, LLM call limits, and agent transfer if needed.
   *
   * @return A {@link Flowable} of {@link Event} objects from this step.
   * @throws LlmCallsLimitExceededException if the agent exceeds allowed LLM invocations.
   * @throws IllegalStateException if a transfer agent is specified but not found.
   */
  private Flowable<Event> runOneStep(InvocationContext context) {
    AtomicReference<LlmRequest> llmRequestRef = new AtomicReference<>(LlmRequest.builder().build());

    return Flowable.defer(
        () -> {
          Context currentContext = Context.current();
          return preprocess(context, llmRequestRef)
              .concatWith(
                  Flowable.defer(
                      () -> {
                        LlmRequest llmRequestAfterPreprocess = llmRequestRef.get();
                        if (context.endInvocation()) {
                          logger.debug("End invocation requested during preprocessing.");
                          return Flowable.empty();
                        }

                        try {
                          context.incrementLlmCallsCount();
                        } catch (LlmCallsLimitExceededException e) {
                          logger.error("LLM calls limit exceeded.", e);
                          return Flowable.error(e);
                        }

                        final Event mutableEventTemplate =
                            Event.builder()
                                .id(Event.generateEventId())
                                .invocationId(context.invocationId())
                                .author(context.agent().name())
                                .branch(context.branch())
                                .build();
                        mutableEventTemplate.setTimestamp(0L);

                        return callLlm(context, llmRequestAfterPreprocess, mutableEventTemplate)
                            .concatMap(
                                llmResponse -> {
                                  try (Scope postScope = currentContext.makeCurrent()) {
                                    return postprocess(
                                            context,
                                            mutableEventTemplate,
                                            llmRequestAfterPreprocess,
                                            llmResponse)
                                        .doFinally(
                                            () -> {
                                              String oldId = mutableEventTemplate.id();
                                              String newId = Event.generateEventId();
                                              logger.debug(
                                                  "Resetting event ID from {} to {}", oldId, newId);
                                              mutableEventTemplate.setId(newId);
                                            });
                                  }
                                })
                            .concatMap(
                                event -> {
                                  Flowable<Event> postProcessedEvents = Flowable.just(event);
                                  if (event.actions().transferToAgent().isPresent()) {
                                    String agentToTransfer =
                                        event.actions().transferToAgent().get();
                                    BaseAgent rootAgent = context.agent().rootAgent();
                                    Optional<BaseAgent> nextAgent =
                                        rootAgent.findAgent(agentToTransfer);
                                    if (nextAgent.isEmpty()) {
                                      logger.error("Agent not found: {}", agentToTransfer);
                                      return postProcessedEvents.concatWith(
                                          Flowable.error(
                                              new IllegalStateException(
                                                  "Agent not found: " + agentToTransfer)));
                                    }
                                    return postProcessedEvents.concatWith(
                                        Flowable.defer(() -> nextAgent.get().runAsync(context)));
                                  }
                                  return postProcessedEvents;
                                });
                      }));
        });
  }

  /**
   * Executes the full LLM flow by repeatedly calling {@link #runOneStep} until a final response is
   * produced.
   *
   * @return A {@link Flowable} of all {@link Event}s generated during the flow.
   */
  @Override
  public Flowable<Event> run(InvocationContext invocationContext) {
    return run(invocationContext, 0);
  }

  private Flowable<Event> run(InvocationContext invocationContext, int stepsCompleted) {
    Flowable<Event> currentStepEvents = runOneStep(invocationContext).cache();
    if (stepsCompleted + 1 >= maxSteps) {
      logger.debug("Ending flow execution because max steps reached.");
      return currentStepEvents;
    }

    return currentStepEvents.concatWith(
        currentStepEvents
            .toList()
            .flatMapPublisher(
                eventList -> {
                  if (eventList.isEmpty()
                      || Iterables.getLast(eventList).finalResponse()
                      || Iterables.getLast(eventList).actions().endInvocation().orElse(false)) {
                    logger.debug(
                        "Ending flow execution based on final response, endInvocation action or"
                            + " empty event list.");
                    return Flowable.empty();
                  } else {
                    logger.debug("Continuing to next step of the flow.");
                    return run(invocationContext, stepsCompleted + 1);
                  }
                }));
  }

  /**
   * Executes the LLM flow in streaming mode.
   *
   * <p>Handles sending history and live requests to the LLM, receiving responses, processing them,
   * and managing agent transfers.
   *
   * @return A {@link Flowable} of {@link Event}s streamed in real-time.
   */
  @Override
  public Flowable<Event> runLive(InvocationContext invocationContext) {
    AtomicReference<LlmRequest> llmRequestRef = new AtomicReference<>(LlmRequest.builder().build());
    Flowable<Event> preprocessEvents = preprocess(invocationContext, llmRequestRef);

    return preprocessEvents.concatWith(
        Flowable.defer(
            () -> {
              LlmRequest llmRequestAfterPreprocess = llmRequestRef.get();
              if (invocationContext.endInvocation()) {
                return Flowable.empty();
              }

              String eventIdForSendData = Event.generateEventId();
              LlmAgent agent = (LlmAgent) invocationContext.agent();
              BaseLlm llm =
                  agent.resolvedModel().model().isPresent()
                      ? agent.resolvedModel().model().get()
                      : LlmRegistry.getLlm(agent.resolvedModel().modelName().get());
              BaseLlmConnection connection = llm.connect(llmRequestAfterPreprocess);
              Completable historySent =
                  llmRequestAfterPreprocess.contents().isEmpty()
                      ? Completable.complete()
                      : connection
                          .sendHistory(llmRequestAfterPreprocess.contents())
                          .doOnComplete(
                              () ->
                                  Tracing.traceSendData(
                                      invocationContext,
                                      eventIdForSendData,
                                      llmRequestAfterPreprocess.contents()))
                          .doOnError(
                              error -> {
                                Span span = Span.current();
                                span.setStatus(StatusCode.ERROR, error.getMessage());
                                span.recordException(error);
                                Tracing.traceSendData(
                                    invocationContext,
                                    eventIdForSendData,
                                    llmRequestAfterPreprocess.contents());
                              })
                          .compose(Tracing.trace("send_data"));

              Flowable<LiveRequest> liveRequests =
                  invocationContext
                      .liveRequestQueue()
                      .get()
                      .get()
                      .doOnNext(
                          request -> {
                            if (!invocationContext.activeStreamingTools().isEmpty()) {
                              for (ActiveStreamingTool activeStreamingTool :
                                  invocationContext.activeStreamingTools().values()) {
                                if (activeStreamingTool.stream() != null) {
                                  activeStreamingTool.stream().send(request);
                                }
                              }
                            }
                          });
              Disposable sendTask =
                  historySent
                      .observeOn(agent.executor().map(Schedulers::from).orElse(Schedulers.io()))
                      .andThen(
                          liveRequests
                              .onBackpressureBuffer()
                              .concatMapCompletable(
                                  request -> {
                                    if (request.content().isPresent()) {
                                      return connection.sendContent(request.content().get());
                                    } else if (request.blob().isPresent()) {
                                      return connection.sendRealtime(request.blob().get());
                                    }
                                    return Completable.fromAction(connection::close);
                                  }))
                      .subscribeWith(
                          new DisposableCompletableObserver() {
                            @Override
                            public void onComplete() {
                              connection.close();
                            }

                            @Override
                            public void onError(Throwable e) {
                              connection.close(e);
                            }
                          });

              Event.Builder liveEventBuilderTemplate =
                  Event.builder()
                      .invocationId(invocationContext.invocationId())
                      .author(invocationContext.agent().name())
                      .branch(invocationContext.branch());

              Flowable<Event> receiveFlow =
                  connection
                      .receive()
                      .flatMap(
                          llmResponse -> {
                            Event baseEventForThisLlmResponse =
                                liveEventBuilderTemplate.id(Event.generateEventId()).build();
                            return postprocess(
                                invocationContext,
                                baseEventForThisLlmResponse,
                                llmRequestAfterPreprocess,
                                llmResponse);
                          })
                      .flatMap(
                          event -> {
                            Flowable<Event> events = Flowable.just(event);
                            if (event.actions().transferToAgent().isPresent()) {
                              BaseAgent rootAgent = invocationContext.agent().rootAgent();
                              Optional<BaseAgent> nextAgent =
                                  rootAgent.findAgent(event.actions().transferToAgent().get());
                              if (nextAgent.isEmpty()) {
                                throw new IllegalStateException(
                                    "Agent not found: " + event.actions().transferToAgent().get());
                              }
                              Flowable<Event> nextAgentEvents =
                                  nextAgent.get().runLive(invocationContext);
                              events = Flowable.concat(events, nextAgentEvents);
                            }
                            return events;
                          })
                      .doOnNext(
                          event -> {
                            ImmutableList<FunctionResponse> functionResponses =
                                event.functionResponses();
                            if (!functionResponses.isEmpty()) {
                              invocationContext
                                  .liveRequestQueue()
                                  .get()
                                  .content(event.content().get());
                            }
                            if (functionResponses.stream()
                                    .anyMatch(
                                        functionResponse ->
                                            functionResponse
                                                .name()
                                                .orElse("")
                                                .equals("transferToAgent"))
                                || event.actions().endInvocation().orElse(false)) {
                              sendTask.dispose();
                              connection.close();
                            }
                          });

              return receiveFlow.takeWhile(event -> !event.actions().endInvocation().orElse(false));
            }));
  }

  /**
   * Builds an {@link Event} from LLM response, request, and base event data.
   *
   * <p>Populates the event with LLM output and tool function call metadata.
   *
   * @return A fully constructed {@link Event} representing the LLM response.
   */
  private Flowable<Event> buildPostprocessingEvents(
      LlmResponse updatedResponse,
      List<Iterable<Event>> eventIterables,
      InvocationContext context,
      Event baseEventForLlmResponse,
      LlmRequest llmRequest) {
    Flowable<Event> processorEvents = Flowable.fromIterable(Iterables.concat(eventIterables));
    if (updatedResponse.content().isEmpty()
        && updatedResponse.errorCode().isEmpty()
        && !updatedResponse.interrupted().orElse(false)
        && !updatedResponse.turnComplete().orElse(false)) {
      return processorEvents;
    }

    Event modelResponseEvent =
        buildModelResponseEvent(baseEventForLlmResponse, llmRequest, updatedResponse);
    if (modelResponseEvent.functionCalls().isEmpty()) {
      return processorEvents.concatWith(Flowable.just(modelResponseEvent));
    }

    Maybe<Event> maybeFunctionResponseEvent =
        context.runConfig().streamingMode() == StreamingMode.BIDI
            ? Functions.handleFunctionCallsLive(context, modelResponseEvent, llmRequest.tools())
            : Functions.handleFunctionCalls(context, modelResponseEvent, llmRequest.tools());

    Flowable<Event> functionEvents =
        maybeFunctionResponseEvent.flatMapPublisher(
            functionResponseEvent -> {
              Optional<Event> toolConfirmationEvent =
                  Functions.generateRequestConfirmationEvent(
                      context, modelResponseEvent, functionResponseEvent);
              return toolConfirmationEvent.isPresent()
                  ? Flowable.just(toolConfirmationEvent.get(), functionResponseEvent)
                  : Flowable.just(functionResponseEvent);
            });

    return processorEvents.concatWith(Flowable.just(modelResponseEvent)).concatWith(functionEvents);
  }

  private Event buildModelResponseEvent(
      Event baseEventForLlmResponse, LlmRequest llmRequest, LlmResponse llmResponse) {
    Event.Builder eventBuilder =
        baseEventForLlmResponse.toBuilder()
            .content(llmResponse.content())
            .partial(llmResponse.partial())
            .errorCode(llmResponse.errorCode())
            .errorMessage(llmResponse.errorMessage())
            .interrupted(llmResponse.interrupted())
            .turnComplete(llmResponse.turnComplete())
            .groundingMetadata(llmResponse.groundingMetadata())
            .avgLogprobs(llmResponse.avgLogprobs())
            .finishReason(llmResponse.finishReason())
            .usageMetadata(llmResponse.usageMetadata())
            .modelVersion(llmResponse.modelVersion());

    Event event = eventBuilder.build();

    logger.debug("event: {} functionCalls: {}", event, event.functionCalls());

    if (!event.functionCalls().isEmpty()) {
      Functions.populateClientFunctionCallId(event);
      Set<String> longRunningToolIds =
          Functions.getLongRunningFunctionCalls(event.functionCalls(), llmRequest.tools());
      logger.debug("longRunningToolIds: {}", longRunningToolIds);
      if (!longRunningToolIds.isEmpty()) {
        event.setLongRunningToolIds(Optional.of(longRunningToolIds));
      }
    }
    return event;
  }
}
